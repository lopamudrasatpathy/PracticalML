hello()
library(ggplot2)
#MACHINE LEARNING MODEL(WAGE DATA)
library(ISLR)
#cut2 ,making factors (Hmisc package)(here g = group of dataset)
library(Hmisc)
#Boxplot with cut2 (need lattice and survival package)
install.packages("gridExtra")
install.packages("gridExtra")
#MACHINE LEARNING MODEL(WAGE DATA)
library(ISLR)
library(ggplot2)
library(caret)
data("Wage")
head(Wage)
dim(Wage)
complete.cases(Wage)
summary(Wage)
#Getting training and testing sets
inTrain <- createDataPartition(y = Wage$wage,p=0.75,list = FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training)
dim(testing)
#Feature plot(a shortcut to produce lattice plot)
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot = "pairs")
qplot(age,wage,color=jobclass,data = training)
#Add Regression smoothers
qq <- qplot(age,wage,color = education,data = training)
qq + geom_smooth(method = "lm",formula = y ~ x)
#cut2 ,making factors (Hmisc package)(here g = group of dataset)
library(Hmisc)
cutWage <- cut2(training$wage,g=3)
table(cutWage)
#Boxplot with cut2 (need lattice and survival package)
install.packages("gridExtra")
library(gridExtra)
p1 <- qplot(cutWage,age,data = training, fill=cutWage,geom = c("boxplot","jitter"))
rm(list = ls())
setwd("C:/Users/lopam/Desktop/coursera/practical machine learning")
#Installation of required packages
install.packages("caret")
install.packages("knitr")
install.packages("ggplot2")
install.packages("lattice")
install.packages("corrplot")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("rattle")
library(caret)
library(ggplot2)
library(lattice)
library(knitr)
library(corrplot)
library(rpart)
library(rpart.plot)
library(rattle)
#Loading of Dataset:
trainurl = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl= "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainData <- read.csv(url(trainurl),header = TRUE)
testData <- read.csv(url(testurl),header = TRUE)
#Partition of Dataset:
inTrain <- createDataPartition(trainData$classe,p = 0.7,list=FALSE)
training <- trainData[inTrain,]
testing <- trainData[-inTrain,]
head(training)
names(training)
dim(training)
complete.cases(training)
#Imputation of NA with the mean value
NZV <- nearZeroVar(training)
training <- training[,-NZV]
testing <- testing[,-NZV]
dim(training)
dim(testing)
AllNA <-sapply(training,function(x) mean(is.na(x)))
training <- training[,AllNA == FALSE]
testing <- testing[,AllNA == FALSE]
dim(training)
dim(testing)
str(training)
#Removing first 5 features from the training  and testing dataset and Now number of variables are reduced to 54.
training <- training[,-(1:5)]
testing <- testing[,-(1:5)]
dim(training)
dim(testing)
#Correlation Analysis
corMatrix <- cor(training[,-54])
corrplot(corMatrix, order = "FPC", method = "color", type = "lower", tl.cex =0.8, tl.col =rgb(0,0,0))
###Prediction Model :
##1. Generalised Boosting Model(GBM)
set.seed(32343)
controlGBM <-trainControl(method = "repeatedcv", number = 10,repeats = 1)
ModelFitGBM <- train(classe ~., data = training, method = "gbm", trControl= controlGBM, verboseIter = FALSE )
print(ModelFitGBM$finalModel)
#Prediction on Testset
predictionsGBM <- predict(ModelFitGBM, newdata = testing)
confusionMatrix(predictionsGBM, testing$classe)
plot(ModelFitGBM,ylim= c(0.7,1))
##2.Random Forest Model
set.seed(12345)
controlRF <- trainControl(method = "repeatedcv", number = 10,repeats = 1)
ModelFitRF <- train(classe ~., data = training, method ="rf",trControl = controlRF, verboseIter = FALSE)
print(ModelFitRF$finalModel)
#Prediction on Testset
predictionsRF <- predict(ModelFitRF, newdata = testing)
confusionMatrix(predictionsRF, testing$classe)
plot(ModelFitRF)
##3. Decision Tree Model
set.seed(12345)
ModelFitDT <- train(classe~.,data = training, method = "rpart")
print(ModelFitDT$finalModel)
#Prediction on Testset
predictionDT <- predict(ModelFitDT, newdata = testing)
confusionMatrix(predictionDT, testing$classe)
fancyRpartPlot(ModelFitDT$finalModel)
##APPLYING THE SELECTED MODEL TO THE TEST DATA
#The accuracy for the 3 models are:
#1. GENERALL BOOSTING MODEL(GBM) : 0.9912
#2. RANDOM FOREST MODEL(RF):0.9978
#3. DECISION TREES (DT):0.4952
#In this case, Random Forest model will be applied to predict 20 quiz results on testing dataset.
PredictionTest <- predict(ModelFitRF, testing, type = "class")
predictionTest
install.packages("ggplot2")
install.packages("ggplot2")
set.seed(32343)
controlGBM <-trainControl(method = "repeatedcv", number = 10,repeats = 1)
ModelFitGBM <- train(classe ~., data = training, method = "gbm", trControl= controlGBM, verboseIter = FALSE )
ModelFitGBM <- train(classe~., data = training, method = "gbm", trControl= controlGBM, verboseIter = FALSE)
ModelFitGBM <- train(classe~., data = training, method = "gbm", trControl= controlGBM, verbose = FALSE)
set.seed(32343)
controlGBM <-trainControl(method = "repeatedcv", number = 5,repeats = 1)
ModelFitGBM <- train(classe~., data = training, method = "gbm", trControl= controlGBM, verbose = FALSE)
print(ModelFitGBM$finalModel)
predictionsGBM <- predict(ModelFitGBM, newdata = testing)
confusionMatrix(predictionsGBM, testing$classe)
plot(ModelFitGBM,ylim= c(0.7,1))
set.seed(12345)
controlRF <- trainControl(method = "repeatedcv", number = 5,repeats = 1)
ModelFitRF <- train(classe~., data = training, method ="rf",trControl = controlRF, verbose = FALSE)
print(ModelFitRF$finalModel)
set.seed(32343)
controlGBM <-trainControl(method = "cv", number = 5,repeats = 1)
ModelFitGBM <- train(classe~., data = training, method = "gbm", trControl= controlGBM, verbose = FALSE)
print(ModelFitGBM$finalModel)
set.seed(32343)
controlGBM <-trainControl(method = "cv", number = 5)
ModelFitGBM <- train(classe~., data = training, method = "gbm", trControl= controlGBM, verbose = FALSE)
print(ModelFitGBM$finalModel)
predictionsGBM <- predict(ModelFitGBM, newdata = testing)
confusionMatrix(predictionsGBM, testing$classe)
plot(ModelFitGBM,ylim= c(0.7,1))
set.seed(12345)
controlRF <- trainControl(method = "cv", number = 5)
ModelFitRF <- train(classe~., data = training, method ="rf",trControl = controlRF, verbose = FALSE)
print(ModelFitRF$finalModel)
#Prediction on Testset
predictionsRF <- predict(ModelFitRF, newdata = testing)
confusionMatrix(predictionsRF, testing$classe)
plot(ModelFitRF)
set.seed(12345)
ModelFitDT <- train(classe~.,data = training, method = "rpart")
print(ModelFitDT$finalModel)
predictionDT <- predict(ModelFitDT, newdata = testing)
confusionMatrix(predictionDT, testing$classe)
fancyRpartPlot(ModelFitDT$finalModel)
##APPLYING THE SELECTED MODEL TO THE TEST DATA
#The accuracy for the 3 models are:
#1. GENERALL BOOSTING MODEL(GBM) : 0.99
#2. RANDOM FOREST MODEL(RF):0.9973
#3. DECISION TREES (DT):0.4952
#In this case, Random Forest model will be applied to predict 20 quiz results on testing dataset.
PredictionTest <- predict(ModelFitRF, testing, type = "class")
##APPLYING THE SELECTED MODEL TO THE TEST DATA
#The accuracy for the 3 models are:
#1. GENERALL BOOSTING MODEL(GBM) : 0.99
#2. RANDOM FOREST MODEL(RF):0.9973
#3. DECISION TREES (DT):0.4952
#In this case, Random Forest model will be applied to predict 20 quiz results on testing dataset.
PredictionTest <- predict(ModelFitRF, testing$classe)
##APPLYING THE SELECTED MODEL TO THE TEST DATA
#The accuracy for the 3 models are:
#1. GENERALL BOOSTING MODEL(GBM) : 0.99
#2. RANDOM FOREST MODEL(RF):0.9973
#3. DECISION TREES (DT):0.4952
#In this case, Random Forest model will be applied to predict 20 quiz results on testing dataset.
PredictionTest <- predict(ModelFitRF, newdata = testing)
predictionTest
predictionTest
##APPLYING THE SELECTED MODEL TO THE TEST DATA
#The accuracy for the 3 models are:
#1. GENERALL BOOSTING MODEL(GBM) : 0.99
#2. RANDOM FOREST MODEL(RF):0.9973
#3. DECISION TREES (DT):0.4952
#In this case, Random Forest model will be applied to predict 20 quiz results on testing dataset.
predictionTest <- predict(ModelFitRF, newdata = testing)
predictionTest
#Imputation of NA with the mean value
NZV <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[,-NZV]
training <- training[,-NZV]
#Imputation of NA with the mean value
NZV <- nearZeroVar(trainData, saveMetrics = TRUE)
trainData <- trainData[ ,NZV$nzv==FALSE]
NZV1 <- nearZeroVar(testData, saveMetrics = TRUE)
testData <- testData[,NZV1$nzv==FALSE]
AllNA <-sapply(trainData,function(x) mean(is.na(x)))
training <- trainData[,AllNA == FALSE]
testing <- testData[,AllNA == FALSE]
testing <- testData[,AllNA == FALSE]
AllNA1 <- sapply(testData, function(x)mean(is.na(x)))
testing <-  testData[ ,AllNA1 == FALSE]
AllNA <-sapply(trainData,function(x) mean(is.na(x)))
trainData <- trainData[,AllNA == FALSE]
AllNA1 <- sapply(testData, function(x)mean(is.na(x)))
testData <-  testData[ ,AllNA1 == FALSE]
#Removing first 5 features from the training  and testing dataset and Now number of variables are reduced to 54.
training <- trainData[,-(1:5)]
testing <- testData[,-(1:5)]
dim(training)
dim(testing)
trainData <- trainData[,-(1:5)]
testData <- testData[,-(1:5)]
dim(trainData)
dim(testData)
#Partition of Dataset:
inTrain <- createDataPartition(trainData$classe,p = 0.7,list=FALSE)
training <- trainData[inTrain,]
testing <- trainData[-inTrain,]
head(training)
dim(training)
corMatrix <- cor(training[,-54])
corrplot(corMatrix, order = "FPC", method = "color", type = "lower", tl.cex =0.8, tl.col =rgb(0,0,0))
set.seed(12345)
controlGBM <-trainControl(method = "cv", number = 5)
ModelFitGBM <- train(classe~., data = training, method = "gbm", trControl= controlGBM, verbose = FALSE)
print(ModelFitGBM$finalModel)
predictionsGBM <- predict(ModelFitGBM, newdata = testing)
confusionMatrix(predictionsGBM, testing$classe)
plot(ModelFitGBM,ylim= c(0.7,1))
set.seed(12345)
controlRF <- trainControl(method = "cv", number = 5)
ModelFitRF <- train(classe~., data = training, method ="rf",trControl = controlRF, verbose = FALSE)
print(ModelFitRF$finalModel)
predictionsRF <- predict(ModelFitRF, newdata = testing)
confusionMatrix(predictionsRF, testing$classe)
plot(ModelFitRF)
set.seed(12345)
ModelFitDT <- train(classe~.,data = training, method = "rpart")
print(ModelFitDT$finalModel)
predictionDT <- predict(ModelFitDT, newdata = testing)
confusionMatrix(predictionDT, testing$classe)
fancyRpartPlot(ModelFitDT$finalModel)
predictionTest <- predict(ModelFitRF, newdata = testData)
predictionTest
##APPLYING THE SELECTED MODEL TO THE TEST DATA
#The accuracy for the 3 models are:
#1. GENERALL BOOSTING MODEL(GBM) : 0.9856
#2. RANDOM FOREST MODEL(RF):0.9985
#3. DECISION TREES (DT):0.53
# Best Model : Random forest Model
Accuracy <- postResample(predictionsRF,testData$classe)
##APPLYING THE SELECTED MODEL TO THE TEST DATA
#The accuracy for the 3 models are:
#1. GENERALL BOOSTING MODEL(GBM) : 0.9856
#2. RANDOM FOREST MODEL(RF):0.9985
#3. DECISION TREES (DT):0.53
# Best Model : Random forest Model
Accuracy <- postResample(predictionsRF,newdata = testData)
##APPLYING THE SELECTED MODEL TO THE TEST DATA
#The accuracy for the 3 models are:
#1. GENERALL BOOSTING MODEL(GBM) : 0.9856
#2. RANDOM FOREST MODEL(RF):0.9985
#3. DECISION TREES (DT):0.53
# Best Model : Random forest Model
Accuracy <- postResample(predictionsRF,testing$classe)
Accuracy
OoSerror <- 1 - as.numeric(confusionMatrix(testing$classe, predictionsRF)$overall[1])
OoSerror
install.packages("tree")
library(tree)
#2. Random Forest Model(tree model)
tree(classe~.,data = training)
#2. Random Forest Model(tree model)
rftree <- tree(classe~.,data = training)
rftree$frame
install.packages("randomForest")
install.packages("randomForest")
library(randomForest)
library(randomForest)
prp(rftree)
rftree1 <- rpart(classe~., data = training, method = "class")
prp(rftree1)
setwd("C:/Users/lopam/Desktop/coursera/practical machine learning")
setwd("C:/Users/lopam/Desktop/coursera/practical machine learning")
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(ggplot2)
library(lattice)
library(knitr)
library(corrplot)
library(rpart)
library(rpart.plot)
library(rattle)
library(tree)
trainurl = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl= "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainData <- read.csv(url(trainurl),header = TRUE)
testData <- read.csv(url(testurl),header = TRUE)
NZV <- nearZeroVar(trainData, saveMetrics = TRUE)
NZV1 <- nearZeroVar(testData, saveMetrics = TRUE)
trainData <- trainData[ ,NZV$nzv==FALSE]
testData <- testData[ ,NZV1$nzv==FALSE]
AllNA <-sapply(trainData,function(x) mean(is.na(x)))
AllNA1 <- sapply(testData, function(x)mean(is.na(x)))
trainData <- trainData[,AllNA == FALSE]
testData <-  testData[ ,AllNA1 == FALSE]
trainData <- trainData[,-(1:5)]
testData <- testData[,-(1:5)]
dim(trainData)
dim(testData)
inTrain <- createDataPartition(trainData$classe,p = 0.7,list=FALSE)
training <- trainData[inTrain,]
testing <- trainData[-inTrain,]
corMatrix <- cor(training[,-54])
corrplot(corMatrix, order = "FPC", method = "color", type = "lower", tl.cex =0.8, tl.col =rgb(0,0,0))
set.seed(12345)
controlGBM <-trainControl(method = "cv", number = 5)
ModelFitGBM <- train(classe~., data = training, method = "gbm", trControl= controlGBM, verbose = FALSE)
print(ModelFitGBM$finalModel)
predictionsGBM <- predict(ModelFitGBM, newdata = testing)
confusionMatrix(predictionsGBM, testing$classe)
plot(ModelFitGBM,ylim= c(0.7,1))
set.seed(12345)
controlRF <- trainControl(method = "cv", number = 5)
ModelFitRF <- train(classe~., data = training, method ="rf",trControl = controlRF, verbose = FALSE)
print(ModelFitRF$finalModel)
predictionsRF <- predict(ModelFitRF, newdata = testing)
confusionMatrix(predictionsRF, testing$classe)
plot(ModelFitRF)
set.seed(12345)
ModelFitDT <- train(classe~.,data = training, method = "rpart")
print(ModelFitDT$finalModel)
predictionDT <- predict(ModelFitDT, newdata = testing)
confusionMatrix(predictionDT, testing$classe)
fancyRpartPlot(ModelFitDT$finalModel)
Accuracy <- postResample(predictionsRF,testing$classe)
Accuracy
OoSerror <- 1 - as.numeric(confusionMatrix(testing$classe, predictionsRF)$overall[1])
OoSerror
predictionTest <- predict(ModelFitRF, newdata = testData)
predictionTest
corrplot(corMatrix, order = "FPC", method = "color", type = "lower", tl.cex =0.8, tl.col =rgb(0,0,0))
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(ggplot2)
library(lattice)
library(knitr)
library(corrplot)
library(rpart)
library(rpart.plot)
library(rattle)
library(tree)
trainurl = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl= "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainData <- read.csv(url(trainurl),header = TRUE)
testData <- read.csv(url(testurl),header = TRUE)
NZV <- nearZeroVar(trainData, saveMetrics = TRUE)
NZV1 <- nearZeroVar(testData, saveMetrics = TRUE)
trainData <- trainData[ ,NZV$nzv==FALSE]
testData <- testData[ ,NZV1$nzv==FALSE]
AllNA <-sapply(trainData,function(x) mean(is.na(x)))
AllNA1 <- sapply(testData, function(x)mean(is.na(x)))
trainData <- trainData[,AllNA == FALSE]
testData <-  testData[ ,AllNA1 == FALSE]
trainData <- trainData[,-(1:5)]
testData <- testData[,-(1:5)]
dim(trainData)
dim(testData)
inTrain <- createDataPartition(trainData$classe,p = 0.7,list=FALSE)
training <- trainData[inTrain,]
testing <- trainData[-inTrain,]
corMatrix <- cor(training[,-54])
corrplot(corMatrix, order = "FPC", method = "color", type = "lower", tl.cex =0.8, tl.col =rgb(0,0,0))
set.seed(12345)
controlGBM <-trainControl(method = "cv", number = 5)
ModelFitGBM <- train(classe~., data = training, method = "gbm", trControl= controlGBM, verbose = FALSE)
print(ModelFitGBM$finalModel)
predictionsGBM <- predict(ModelFitGBM, newdata = testing)
confusionMatrix(predictionsGBM, testing$classe)
plot(ModelFitGBM,ylim= c(0.7,1))
set.seed(12345)
controlRF <- trainControl(method = "cv", number = 5)
ModelFitRF <- train(classe~., data = training, method ="rf",trControl = controlRF, verbose = FALSE)
print(ModelFitRF$finalModel)
predictionsRF <- predict(ModelFitRF, newdata = testing)
confusionMatrix(predictionsRF, testing$classe)
plot(ModelFitRF)
set.seed(12345)
ModelFitDT <- train(classe~.,data = training, method = "rpart")
print(ModelFitDT$finalModel)
predictionDT <- predict(ModelFitDT, newdata = testing)
confusionMatrix(predictionDT, testing$classe)
fancyRpartPlot(ModelFitDT$finalModel)
Accuracy <- postResample(predictionsRF,testing$classe)
Accuracy
OoSerror <- 1 - as.numeric(confusionMatrix(testing$classe, predictionsRF)$overall[1])
OoSerror
predictionTest <- predict(ModelFitRF, newdata = testData)
predictionTest
corrplot(corMatrix, order = "FPC", method = "color", type = "lower", tl.cex =0.8, tl.col =rgb(0,0,0))
rftree <- tree(classe~.,data = training)
rftree$frame
rftree <- tree(classe~.,data = training)
rftree$frame
rftree <- tree(classe~.,data = training)
rftree$frame
rftree <- tree(classe~.,data = training)
rftree$frame
library(randomForest)
install.packages("randomForest")
install.packages("randomForest")
library(randomForest)
